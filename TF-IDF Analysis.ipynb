{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types, functions as F\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Messenger TF-IDF Analysis\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "IGNORES = \"./ignore.txt\"\n",
    "DATA_ROOT = \"./data/messages\"\n",
    "filepaths = list(pathname for pathname in glob.iglob(f'{DATA_ROOT}/**/message.json', recursive=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- is_still_participant: boolean (nullable = true)\n",
      " |-- messages: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- audio_files: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- creation_timestamp: long (nullable = true)\n",
      " |    |    |    |    |-- uri: string (nullable = true)\n",
      " |    |    |-- call_duration: long (nullable = true)\n",
      " |    |    |-- content: string (nullable = true)\n",
      " |    |    |-- files: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- creation_timestamp: long (nullable = true)\n",
      " |    |    |    |    |-- uri: string (nullable = true)\n",
      " |    |    |-- gifs: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- uri: string (nullable = true)\n",
      " |    |    |-- missed: boolean (nullable = true)\n",
      " |    |    |-- payment_info: struct (nullable = true)\n",
      " |    |    |    |-- amount: long (nullable = true)\n",
      " |    |    |    |-- completedTime: long (nullable = true)\n",
      " |    |    |    |-- creationTime: long (nullable = true)\n",
      " |    |    |    |-- currency: string (nullable = true)\n",
      " |    |    |    |-- receiverName: string (nullable = true)\n",
      " |    |    |    |-- senderName: string (nullable = true)\n",
      " |    |    |-- photos: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- creation_timestamp: long (nullable = true)\n",
      " |    |    |    |    |-- uri: string (nullable = true)\n",
      " |    |    |-- plan: struct (nullable = true)\n",
      " |    |    |    |-- location: string (nullable = true)\n",
      " |    |    |    |-- timestamp: long (nullable = true)\n",
      " |    |    |    |-- title: string (nullable = true)\n",
      " |    |    |-- reactions: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- actor: string (nullable = true)\n",
      " |    |    |    |    |-- reaction: string (nullable = true)\n",
      " |    |    |-- sender_name: string (nullable = true)\n",
      " |    |    |-- share: struct (nullable = true)\n",
      " |    |    |    |-- link: string (nullable = true)\n",
      " |    |    |    |-- share_text: string (nullable = true)\n",
      " |    |    |-- sticker: struct (nullable = true)\n",
      " |    |    |    |-- uri: string (nullable = true)\n",
      " |    |    |-- timestamp_ms: long (nullable = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |    |-- users: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- videos: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- creation_timestamp: long (nullable = true)\n",
      " |    |    |    |    |-- thumbnail: struct (nullable = true)\n",
      " |    |    |    |    |    |-- uri: string (nullable = true)\n",
      " |    |    |    |    |-- uri: string (nullable = true)\n",
      " |-- participants: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |-- thread_path: string (nullable = true)\n",
      " |-- thread_type: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source_df = spark.read.json(filepaths, multiLine=True)\n",
    "source_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sanitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- content: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- is_outbound: boolean (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_messages_df = source_df.select(F.explode('messages')) \\\n",
    "                                .select('col.*') \\\n",
    "                                .filter(F.col('content').isNotNull()) \\\n",
    "                                .filter(F.col('timestamp_ms').isNotNull()) \\\n",
    "                                .withColumn('is_outbound',\n",
    "                                            F.when(F.col('sender_name') == 'Il Jae Lee', True) \\\n",
    "                                            .otherwise(False)) \\\n",
    "                                .withColumn('timestamp', F.col('timestamp_ms') / 1000) \\\n",
    "                                .withColumn('year', F.year(F.from_unixtime(F.col(\"timestamp\")))) \\\n",
    "                                .filter(F.col('year') > 2010) \\\n",
    "                                .select('content', 'year', 'is_outbound')\n",
    "text_messages_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_messages_df.toPandas()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Message Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_counts = text_messages_df.groupBy(F.col('year'),\n",
    "                                          F.col('is_outbound')) \\\n",
    "                                    .count() \\\n",
    "                                    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "message_counts_data = sorted(message_counts, key=lambda x: x.year * 10 + x.is_outbound)\n",
    "message_counts_data = list(([k] + [d['count'] for d in g])\n",
    "                           for k, g in \n",
    "                           groupby(message_counts_data, \n",
    "                                   key=lambda x: x.year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Inbound</th>\n",
       "      <th>Outbound</th>\n",
       "      <th>Total</th>\n",
       "      <th>Out_scale</th>\n",
       "      <th>Tot_scale</th>\n",
       "      <th>Out_area</th>\n",
       "      <th>Tot_area</th>\n",
       "      <th>Out_scale_per</th>\n",
       "      <th>Tot_scale_per</th>\n",
       "      <th>Out_area_per</th>\n",
       "      <th>Tot_area_per</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>9423</td>\n",
       "      <td>5894</td>\n",
       "      <td>15317</td>\n",
       "      <td>0.384801</td>\n",
       "      <td>1.580539</td>\n",
       "      <td>0.620323</td>\n",
       "      <td>1.257195</td>\n",
       "      <td>38.480%</td>\n",
       "      <td>158.054%</td>\n",
       "      <td>62.032%</td>\n",
       "      <td>125.719%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>6046</td>\n",
       "      <td>3645</td>\n",
       "      <td>9691</td>\n",
       "      <td>0.376122</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.613288</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>37.612%</td>\n",
       "      <td>100.000%</td>\n",
       "      <td>61.329%</td>\n",
       "      <td>100.000%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>21696</td>\n",
       "      <td>19442</td>\n",
       "      <td>41138</td>\n",
       "      <td>0.472604</td>\n",
       "      <td>4.244970</td>\n",
       "      <td>0.687462</td>\n",
       "      <td>2.060332</td>\n",
       "      <td>47.260%</td>\n",
       "      <td>424.497%</td>\n",
       "      <td>68.746%</td>\n",
       "      <td>206.033%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>20973</td>\n",
       "      <td>17103</td>\n",
       "      <td>38076</td>\n",
       "      <td>0.449181</td>\n",
       "      <td>3.929006</td>\n",
       "      <td>0.670209</td>\n",
       "      <td>1.982172</td>\n",
       "      <td>44.918%</td>\n",
       "      <td>392.901%</td>\n",
       "      <td>67.021%</td>\n",
       "      <td>198.217%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>15452</td>\n",
       "      <td>11643</td>\n",
       "      <td>27095</td>\n",
       "      <td>0.429710</td>\n",
       "      <td>2.795893</td>\n",
       "      <td>0.655523</td>\n",
       "      <td>1.672092</td>\n",
       "      <td>42.971%</td>\n",
       "      <td>279.589%</td>\n",
       "      <td>65.552%</td>\n",
       "      <td>167.209%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>16534</td>\n",
       "      <td>11808</td>\n",
       "      <td>28342</td>\n",
       "      <td>0.416626</td>\n",
       "      <td>2.924569</td>\n",
       "      <td>0.645465</td>\n",
       "      <td>1.710137</td>\n",
       "      <td>41.663%</td>\n",
       "      <td>292.457%</td>\n",
       "      <td>64.547%</td>\n",
       "      <td>171.014%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>43817</td>\n",
       "      <td>31724</td>\n",
       "      <td>75541</td>\n",
       "      <td>0.419957</td>\n",
       "      <td>7.794964</td>\n",
       "      <td>0.648041</td>\n",
       "      <td>2.791946</td>\n",
       "      <td>41.996%</td>\n",
       "      <td>779.496%</td>\n",
       "      <td>64.804%</td>\n",
       "      <td>279.195%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>57408</td>\n",
       "      <td>35752</td>\n",
       "      <td>93160</td>\n",
       "      <td>0.383770</td>\n",
       "      <td>9.613043</td>\n",
       "      <td>0.619492</td>\n",
       "      <td>3.100491</td>\n",
       "      <td>38.377%</td>\n",
       "      <td>961.304%</td>\n",
       "      <td>61.949%</td>\n",
       "      <td>310.049%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Inbound  Outbound  Total  Out_scale  Tot_scale  Out_area  Tot_area  \\\n",
       "Year                                                                       \n",
       "2011     9423      5894  15317   0.384801   1.580539  0.620323  1.257195   \n",
       "2012     6046      3645   9691   0.376122   1.000000  0.613288  1.000000   \n",
       "2013    21696     19442  41138   0.472604   4.244970  0.687462  2.060332   \n",
       "2014    20973     17103  38076   0.449181   3.929006  0.670209  1.982172   \n",
       "2015    15452     11643  27095   0.429710   2.795893  0.655523  1.672092   \n",
       "2016    16534     11808  28342   0.416626   2.924569  0.645465  1.710137   \n",
       "2017    43817     31724  75541   0.419957   7.794964  0.648041  2.791946   \n",
       "2018    57408     35752  93160   0.383770   9.613043  0.619492  3.100491   \n",
       "\n",
       "     Out_scale_per Tot_scale_per Out_area_per Tot_area_per  \n",
       "Year                                                        \n",
       "2011       38.480%      158.054%      62.032%     125.719%  \n",
       "2012       37.612%      100.000%      61.329%     100.000%  \n",
       "2013       47.260%      424.497%      68.746%     206.033%  \n",
       "2014       44.918%      392.901%      67.021%     198.217%  \n",
       "2015       42.971%      279.589%      65.552%     167.209%  \n",
       "2016       41.663%      292.457%      64.547%     171.014%  \n",
       "2017       41.996%      779.496%      64.804%     279.195%  \n",
       "2018       38.377%      961.304%      61.949%     310.049%  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_percent(x):\n",
    "    return '{:.3f}%'.format(x * 100)\n",
    "\n",
    "pd.DataFrame(message_counts_data,\n",
    "             columns=['Year', 'Inbound', 'Outbound']) \\\n",
    "    .set_index('Year') \\\n",
    "    .assign(Total=lambda x: x['Inbound'] + x['Outbound']) \\\n",
    "    .assign(Out_scale=lambda x: x['Outbound'].divide(x['Total']),\n",
    "            Tot_scale=lambda x: x['Total'].divide(x['Total'].min())) \\\n",
    "    .assign(Out_area=lambda x: x['Out_scale'].apply(np.sqrt),\n",
    "            Tot_area=lambda x: x['Tot_scale'].apply(np.sqrt)) \\\n",
    "    .assign(Out_scale_per=lambda x: x['Out_scale'].apply(format_percent),\n",
    "            Tot_scale_per=lambda x: x['Tot_scale'].apply(format_percent)) \\\n",
    "    .assign(Out_area_per=lambda x: x['Out_area'].apply(format_percent),\n",
    "            Tot_area_per=lambda x: x['Tot_area'].apply(format_percent)) \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_outbound_df = text_messages_df.filter(F.col('is_outbound')).drop('is_outbound')\n",
    "text_inbound_df = text_messages_df.filter(~F.col('is_outbound')).drop('is_outbound')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_all_year_df = text_messages_df.groupby(\"year\") \\\n",
    "                                    .agg(F.concat_ws(' ', F.collect_list(F.col('content'))).alias('content'))\n",
    "text_outbound_year_df = text_outbound_df.groupby(\"year\") \\\n",
    "                                    .agg(F.concat_ws(' ', F.collect_list(F.col('content'))).alias('content'))\n",
    "text_inbound_year_df = text_inbound_df.groupby(\"year\") \\\n",
    "                                    .agg(F.concat_ws(' ', F.collect_list(F.col('content'))).alias('content'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/iljae/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/iljae/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/iljae/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/iljae/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown, stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "words = set(brown.words())\n",
    "stopwords = set(stopwords.words('english'))\n",
    "lemma = WordNetLemmatizer()\n",
    "ignorewords = set(word.strip() for word in open(IGNORES, 'r').readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "\n",
    "def tokenize(cont):\n",
    "    raw_tokens = [word.lower().strip()\n",
    "#                     lemma.lemmatize(word.lower().strip())\n",
    "                  for sent in sent_tokenize(cont)\n",
    "                  for word in word_tokenize(sent)]\n",
    "    \n",
    "#     isascii = lambda s: len(s) == len(s.encode())\n",
    "    \n",
    "    return [token for token in raw_tokens\n",
    "            if token \\\n",
    "            and len(token) >= 3\n",
    "            and token not in stopwords \\\n",
    "            and token.isalpha()\n",
    "            and token not in ignorewords \\\n",
    "#             and token in words \\\n",
    "            and not token.replace('.','',1).isdigit()]\n",
    "\n",
    "def transform_to_words(df):\n",
    "    return df.withColumn('words', F.udf(tokenize,\n",
    "                                        types.ArrayType(types.StringType()))(F.col('content')))\n",
    "\n",
    "all_words_df = transform_to_words(text_all_year_df)\n",
    "outbound_words_df = transform_to_words(text_outbound_year_df)\n",
    "inbound_words_df = transform_to_words(text_inbound_year_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_embeddings(df):\n",
    "    cv = CountVectorizer(inputCol=\"words\", outputCol=\"vectors\")\n",
    "    words_model = cv.fit(df)\n",
    "    embedding_df = words_model.transform(df)\n",
    "    return embedding_df, words_model\n",
    "\n",
    "all_embeddings_df, all_words_model = transform_to_embeddings(all_words_df)\n",
    "outbound_embeddings_df, outbound_words_model = transform_to_embeddings(outbound_words_df)\n",
    "inbound_embeddings_df, inbound_words_model = transform_to_embeddings(inbound_words_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_features(df):\n",
    "    idf = IDF(inputCol=\"vectors\", outputCol=\"features\")\n",
    "    idf_model = idf.fit(df)\n",
    "    features_df = idf_model.transform(df)\n",
    "    return features_df\n",
    "    \n",
    "all_features_df = transform_to_features(all_embeddings_df)\n",
    "outbound_features_df = transform_to_features(outbound_embeddings_df)\n",
    "inbound_features_df = transform_to_features(inbound_embeddings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORDS = 15\n",
    "\n",
    "def transform_to_tokens(df):\n",
    "    return df.withColumn('tokens', F.udf(lambda vector: (vector.toArray() * -1).argsort()[:NUM_WORDS].tolist(),\n",
    "                                                  types.ArrayType(types.IntegerType()))(F.col('features')))\n",
    "\n",
    "all_tokens_df = transform_to_tokens(all_features_df)\n",
    "outbound_tokens_df = transform_to_tokens(outbound_features_df)\n",
    "inbound_tokens_df = transform_to_tokens(inbound_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_words(data, model):\n",
    "    return list([d.year] + list(model.vocabulary[idx] for idx in d.tokens)\n",
    "                for d in sorted(data, key=lambda d: d.year))\n",
    "\n",
    "all_words = fetch_words(all_tokens_df.select('year', 'tokens').collect(), all_words_model)\n",
    "outbound_words = fetch_words(outbound_tokens_df.select('year', 'tokens').collect(), outbound_words_model)\n",
    "inbound_words = fetch_words(inbound_tokens_df.select('year', 'tokens').collect(), inbound_words_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>Word 5</th>\n",
       "      <th>Word 6</th>\n",
       "      <th>Word 7</th>\n",
       "      <th>Word 8</th>\n",
       "      <th>Word 9</th>\n",
       "      <th>Word 10</th>\n",
       "      <th>Word 11</th>\n",
       "      <th>Word 12</th>\n",
       "      <th>Word 13</th>\n",
       "      <th>Word 14</th>\n",
       "      <th>Word 15</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>putnam</td>\n",
       "      <td>caleb</td>\n",
       "      <td>dnt</td>\n",
       "      <td>judy</td>\n",
       "      <td>donna</td>\n",
       "      <td>yuna</td>\n",
       "      <td>thts</td>\n",
       "      <td>guyz</td>\n",
       "      <td>bryan</td>\n",
       "      <td>campused</td>\n",
       "      <td>dre</td>\n",
       "      <td>tht</td>\n",
       "      <td>kkkkk</td>\n",
       "      <td>paranormal</td>\n",
       "      <td>jaee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>burial</td>\n",
       "      <td>georgy</td>\n",
       "      <td>funeral</td>\n",
       "      <td>smallnum</td>\n",
       "      <td>norton</td>\n",
       "      <td>boarders</td>\n",
       "      <td>bowl</td>\n",
       "      <td>conner</td>\n",
       "      <td>nayan</td>\n",
       "      <td>kohl</td>\n",
       "      <td>bryan</td>\n",
       "      <td>zaahid</td>\n",
       "      <td>parp</td>\n",
       "      <td>caleb</td>\n",
       "      <td>genki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>moai</td>\n",
       "      <td>mcs</td>\n",
       "      <td>digipen</td>\n",
       "      <td>genki</td>\n",
       "      <td>kinect</td>\n",
       "      <td>mhacks</td>\n",
       "      <td>nawh</td>\n",
       "      <td>getz</td>\n",
       "      <td>donna</td>\n",
       "      <td>damian</td>\n",
       "      <td>snowboarding</td>\n",
       "      <td>esenthel</td>\n",
       "      <td>barrons</td>\n",
       "      <td>stanford</td>\n",
       "      <td>fragments</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>craneshout</td>\n",
       "      <td>lemoneda</td>\n",
       "      <td>unity</td>\n",
       "      <td>domo</td>\n",
       "      <td>rust</td>\n",
       "      <td>nawh</td>\n",
       "      <td>donna</td>\n",
       "      <td>wot</td>\n",
       "      <td>sticker</td>\n",
       "      <td>readly</td>\n",
       "      <td>dcd</td>\n",
       "      <td>launch</td>\n",
       "      <td>kthdigit</td>\n",
       "      <td>cranetalk</td>\n",
       "      <td>snowday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>nick</td>\n",
       "      <td>eric</td>\n",
       "      <td>tony</td>\n",
       "      <td>drone</td>\n",
       "      <td>vaporwave</td>\n",
       "      <td>thipok</td>\n",
       "      <td>torchlight</td>\n",
       "      <td>sticker</td>\n",
       "      <td>jonah</td>\n",
       "      <td>scs</td>\n",
       "      <td>sin</td>\n",
       "      <td>cit</td>\n",
       "      <td>kelly</td>\n",
       "      <td>equipotential</td>\n",
       "      <td>machine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>robin</td>\n",
       "      <td>terrance</td>\n",
       "      <td>nate</td>\n",
       "      <td>beatrix</td>\n",
       "      <td>sophus</td>\n",
       "      <td>noop</td>\n",
       "      <td>gord</td>\n",
       "      <td>tendi</td>\n",
       "      <td>lisa</td>\n",
       "      <td>tender</td>\n",
       "      <td>argo</td>\n",
       "      <td>onboardiq</td>\n",
       "      <td>dolo</td>\n",
       "      <td>housing</td>\n",
       "      <td>dan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>wya</td>\n",
       "      <td>lmao</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>jarrid</td>\n",
       "      <td>perry</td>\n",
       "      <td>jeehee</td>\n",
       "      <td>lmk</td>\n",
       "      <td>salesforce</td>\n",
       "      <td>caltrain</td>\n",
       "      <td>sia</td>\n",
       "      <td>entropy</td>\n",
       "      <td>cristina</td>\n",
       "      <td>theta</td>\n",
       "      <td>whatchu</td>\n",
       "      <td>uber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>wya</td>\n",
       "      <td>jarrid</td>\n",
       "      <td>lmao</td>\n",
       "      <td>hug</td>\n",
       "      <td>anton</td>\n",
       "      <td>lmk</td>\n",
       "      <td>corso</td>\n",
       "      <td>barzan</td>\n",
       "      <td>daston</td>\n",
       "      <td>thach</td>\n",
       "      <td>pooja</td>\n",
       "      <td>maliev</td>\n",
       "      <td>nyc</td>\n",
       "      <td>whatchu</td>\n",
       "      <td>baby</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word 1    Word 2    Word 3    Word 4     Word 5    Word 6  \\\n",
       "Year                                                                  \n",
       "2011      putnam     caleb       dnt      judy      donna      yuna   \n",
       "2012      burial    georgy   funeral  smallnum     norton  boarders   \n",
       "2013        moai       mcs   digipen     genki     kinect    mhacks   \n",
       "2014  craneshout  lemoneda     unity      domo       rust      nawh   \n",
       "2015        nick      eric      tony     drone  vaporwave    thipok   \n",
       "2016       robin  terrance      nate   beatrix     sophus      noop   \n",
       "2017         wya      lmao  accuracy    jarrid      perry    jeehee   \n",
       "2018         wya    jarrid      lmao       hug      anton       lmk   \n",
       "\n",
       "          Word 7      Word 8    Word 9   Word 10       Word 11    Word 12  \\\n",
       "Year                                                                        \n",
       "2011        thts        guyz     bryan  campused           dre        tht   \n",
       "2012        bowl      conner     nayan      kohl         bryan     zaahid   \n",
       "2013        nawh        getz     donna    damian  snowboarding   esenthel   \n",
       "2014       donna         wot   sticker    readly           dcd     launch   \n",
       "2015  torchlight     sticker     jonah       scs           sin        cit   \n",
       "2016        gord       tendi      lisa    tender          argo  onboardiq   \n",
       "2017         lmk  salesforce  caltrain       sia       entropy   cristina   \n",
       "2018       corso      barzan    daston     thach         pooja     maliev   \n",
       "\n",
       "       Word 13        Word 14    Word 15  \n",
       "Year                                      \n",
       "2011     kkkkk     paranormal       jaee  \n",
       "2012      parp          caleb      genki  \n",
       "2013   barrons       stanford  fragments  \n",
       "2014  kthdigit      cranetalk    snowday  \n",
       "2015     kelly  equipotential    machine  \n",
       "2016      dolo        housing        dan  \n",
       "2017     theta        whatchu       uber  \n",
       "2018       nyc        whatchu       baby  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(all_words,\n",
    "             columns=['Year'] + ['Word {}'.format(i+1) for i in range(NUM_WORDS)]) \\\n",
    "    .set_index('Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>Word 5</th>\n",
       "      <th>Word 6</th>\n",
       "      <th>Word 7</th>\n",
       "      <th>Word 8</th>\n",
       "      <th>Word 9</th>\n",
       "      <th>Word 10</th>\n",
       "      <th>Word 11</th>\n",
       "      <th>Word 12</th>\n",
       "      <th>Word 13</th>\n",
       "      <th>Word 14</th>\n",
       "      <th>Word 15</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>caleb</td>\n",
       "      <td>costume</td>\n",
       "      <td>donna</td>\n",
       "      <td>fob</td>\n",
       "      <td>judy</td>\n",
       "      <td>rachel</td>\n",
       "      <td>hao</td>\n",
       "      <td>dre</td>\n",
       "      <td>ziwen</td>\n",
       "      <td>bryan</td>\n",
       "      <td>mudkip</td>\n",
       "      <td>zzzz</td>\n",
       "      <td>xie</td>\n",
       "      <td>goin</td>\n",
       "      <td>bio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>conner</td>\n",
       "      <td>hydrogen</td>\n",
       "      <td>chia</td>\n",
       "      <td>kohl</td>\n",
       "      <td>norton</td>\n",
       "      <td>bon</td>\n",
       "      <td>genki</td>\n",
       "      <td>clay</td>\n",
       "      <td>spectrum</td>\n",
       "      <td>yearbook</td>\n",
       "      <td>holden</td>\n",
       "      <td>religions</td>\n",
       "      <td>carl</td>\n",
       "      <td>allright</td>\n",
       "      <td>smashthewindow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>mcs</td>\n",
       "      <td>moai</td>\n",
       "      <td>joe</td>\n",
       "      <td>genki</td>\n",
       "      <td>mhacks</td>\n",
       "      <td>bham</td>\n",
       "      <td>barrons</td>\n",
       "      <td>fragments</td>\n",
       "      <td>lool</td>\n",
       "      <td>kinect</td>\n",
       "      <td>liz</td>\n",
       "      <td>esenthel</td>\n",
       "      <td>getz</td>\n",
       "      <td>digipen</td>\n",
       "      <td>leap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>craneshout</td>\n",
       "      <td>lemoneda</td>\n",
       "      <td>rust</td>\n",
       "      <td>john</td>\n",
       "      <td>allen</td>\n",
       "      <td>snowday</td>\n",
       "      <td>digitcount</td>\n",
       "      <td>heroku</td>\n",
       "      <td>expansion</td>\n",
       "      <td>launch</td>\n",
       "      <td>probs</td>\n",
       "      <td>kthdigit</td>\n",
       "      <td>cranetalk</td>\n",
       "      <td>domo</td>\n",
       "      <td>theres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>photo</td>\n",
       "      <td>drone</td>\n",
       "      <td>eric</td>\n",
       "      <td>torchlight</td>\n",
       "      <td>kelly</td>\n",
       "      <td>umich</td>\n",
       "      <td>chun</td>\n",
       "      <td>jonah</td>\n",
       "      <td>tony</td>\n",
       "      <td>dope</td>\n",
       "      <td>probs</td>\n",
       "      <td>data</td>\n",
       "      <td>machina</td>\n",
       "      <td>vegan</td>\n",
       "      <td>vaporwave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>sophus</td>\n",
       "      <td>photo</td>\n",
       "      <td>noop</td>\n",
       "      <td>onboardiq</td>\n",
       "      <td>argo</td>\n",
       "      <td>skydiving</td>\n",
       "      <td>housing</td>\n",
       "      <td>san</td>\n",
       "      <td>francisco</td>\n",
       "      <td>yup</td>\n",
       "      <td>montreal</td>\n",
       "      <td>econ</td>\n",
       "      <td>beq</td>\n",
       "      <td>fetch</td>\n",
       "      <td>cpt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>lmao</td>\n",
       "      <td>okie</td>\n",
       "      <td>idk</td>\n",
       "      <td>wya</td>\n",
       "      <td>photo</td>\n",
       "      <td>caltrain</td>\n",
       "      <td>salesforce</td>\n",
       "      <td>uber</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>palo</td>\n",
       "      <td>alto</td>\n",
       "      <td>gpu</td>\n",
       "      <td>hmmm</td>\n",
       "      <td>ugh</td>\n",
       "      <td>branden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>lmao</td>\n",
       "      <td>idk</td>\n",
       "      <td>wya</td>\n",
       "      <td>photo</td>\n",
       "      <td>corso</td>\n",
       "      <td>nyc</td>\n",
       "      <td>tripping</td>\n",
       "      <td>lmk</td>\n",
       "      <td>ehhh</td>\n",
       "      <td>anton</td>\n",
       "      <td>hug</td>\n",
       "      <td>vikas</td>\n",
       "      <td>hmmm</td>\n",
       "      <td>gif</td>\n",
       "      <td>phd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Word 1    Word 2 Word 3      Word 4  Word 5     Word 6      Word 7  \\\n",
       "Year                                                                           \n",
       "2011       caleb   costume  donna         fob    judy     rachel         hao   \n",
       "2012      conner  hydrogen   chia        kohl  norton        bon       genki   \n",
       "2013         mcs      moai    joe       genki  mhacks       bham     barrons   \n",
       "2014  craneshout  lemoneda   rust        john   allen    snowday  digitcount   \n",
       "2015       photo     drone   eric  torchlight   kelly      umich        chun   \n",
       "2016      sophus     photo   noop   onboardiq    argo  skydiving     housing   \n",
       "2017        lmao      okie    idk         wya   photo   caltrain  salesforce   \n",
       "2018        lmao       idk    wya       photo   corso        nyc    tripping   \n",
       "\n",
       "         Word 8     Word 9   Word 10   Word 11    Word 12    Word 13  \\\n",
       "Year                                                                   \n",
       "2011        dre      ziwen     bryan    mudkip       zzzz        xie   \n",
       "2012       clay   spectrum  yearbook    holden  religions       carl   \n",
       "2013  fragments       lool    kinect       liz   esenthel       getz   \n",
       "2014     heroku  expansion    launch     probs   kthdigit  cranetalk   \n",
       "2015      jonah       tony      dope     probs       data    machina   \n",
       "2016        san  francisco       yup  montreal       econ        beq   \n",
       "2017       uber   accuracy      palo      alto        gpu       hmmm   \n",
       "2018        lmk       ehhh     anton       hug      vikas       hmmm   \n",
       "\n",
       "       Word 14         Word 15  \n",
       "Year                            \n",
       "2011      goin             bio  \n",
       "2012  allright  smashthewindow  \n",
       "2013   digipen            leap  \n",
       "2014      domo          theres  \n",
       "2015     vegan       vaporwave  \n",
       "2016     fetch             cpt  \n",
       "2017       ugh         branden  \n",
       "2018       gif             phd  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(outbound_words,\n",
    "             columns=['Year'] + ['Word {}'.format(i+1) for i in range(NUM_WORDS)]) \\\n",
    "    .set_index('Year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>Word 5</th>\n",
       "      <th>Word 6</th>\n",
       "      <th>Word 7</th>\n",
       "      <th>Word 8</th>\n",
       "      <th>Word 9</th>\n",
       "      <th>Word 10</th>\n",
       "      <th>Word 11</th>\n",
       "      <th>Word 12</th>\n",
       "      <th>Word 13</th>\n",
       "      <th>Word 14</th>\n",
       "      <th>Word 15</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011</th>\n",
       "      <td>putnam</td>\n",
       "      <td>tht</td>\n",
       "      <td>dnt</td>\n",
       "      <td>yuna</td>\n",
       "      <td>thts</td>\n",
       "      <td>caleb</td>\n",
       "      <td>kkk</td>\n",
       "      <td>guyz</td>\n",
       "      <td>kkkkk</td>\n",
       "      <td>jaee</td>\n",
       "      <td>doetze</td>\n",
       "      <td>judy</td>\n",
       "      <td>kay</td>\n",
       "      <td>ding</td>\n",
       "      <td>campused</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>burial</td>\n",
       "      <td>georgy</td>\n",
       "      <td>funeral</td>\n",
       "      <td>norton</td>\n",
       "      <td>boarders</td>\n",
       "      <td>smallnum</td>\n",
       "      <td>olympiad</td>\n",
       "      <td>lizzy</td>\n",
       "      <td>nayan</td>\n",
       "      <td>zaahid</td>\n",
       "      <td>bowl</td>\n",
       "      <td>crannet</td>\n",
       "      <td>caleb</td>\n",
       "      <td>scio</td>\n",
       "      <td>gump</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>moai</td>\n",
       "      <td>nawh</td>\n",
       "      <td>donna</td>\n",
       "      <td>digipen</td>\n",
       "      <td>warrant</td>\n",
       "      <td>meixia</td>\n",
       "      <td>mcs</td>\n",
       "      <td>snowboarding</td>\n",
       "      <td>damian</td>\n",
       "      <td>subramanian</td>\n",
       "      <td>peace</td>\n",
       "      <td>julicher</td>\n",
       "      <td>deng</td>\n",
       "      <td>kinect</td>\n",
       "      <td>php</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>lemoneda</td>\n",
       "      <td>craneshout</td>\n",
       "      <td>unity</td>\n",
       "      <td>donna</td>\n",
       "      <td>nawh</td>\n",
       "      <td>wot</td>\n",
       "      <td>lsd</td>\n",
       "      <td>domo</td>\n",
       "      <td>arviso</td>\n",
       "      <td>crannet</td>\n",
       "      <td>sticker</td>\n",
       "      <td>dcd</td>\n",
       "      <td>chayce</td>\n",
       "      <td>dvc</td>\n",
       "      <td>rust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>nick</td>\n",
       "      <td>eric</td>\n",
       "      <td>vaporwave</td>\n",
       "      <td>tony</td>\n",
       "      <td>thipok</td>\n",
       "      <td>sticker</td>\n",
       "      <td>equipotential</td>\n",
       "      <td>drone</td>\n",
       "      <td>ryuni</td>\n",
       "      <td>seizures</td>\n",
       "      <td>recital</td>\n",
       "      <td>kyoto</td>\n",
       "      <td>canon</td>\n",
       "      <td>isoceles</td>\n",
       "      <td>epilepsy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>nate</td>\n",
       "      <td>robin</td>\n",
       "      <td>terrance</td>\n",
       "      <td>beatrix</td>\n",
       "      <td>dan</td>\n",
       "      <td>tendi</td>\n",
       "      <td>tender</td>\n",
       "      <td>gord</td>\n",
       "      <td>octopus</td>\n",
       "      <td>lisa</td>\n",
       "      <td>tahoe</td>\n",
       "      <td>dolo</td>\n",
       "      <td>mission</td>\n",
       "      <td>bootie</td>\n",
       "      <td>perry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>accuracy</td>\n",
       "      <td>perry</td>\n",
       "      <td>wya</td>\n",
       "      <td>jarrid</td>\n",
       "      <td>lmao</td>\n",
       "      <td>jeehee</td>\n",
       "      <td>entropy</td>\n",
       "      <td>sia</td>\n",
       "      <td>lmk</td>\n",
       "      <td>whatchu</td>\n",
       "      <td>cristina</td>\n",
       "      <td>branden</td>\n",
       "      <td>giuseppe</td>\n",
       "      <td>autoencoder</td>\n",
       "      <td>theta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>jarrid</td>\n",
       "      <td>wya</td>\n",
       "      <td>hug</td>\n",
       "      <td>lmao</td>\n",
       "      <td>anton</td>\n",
       "      <td>barzan</td>\n",
       "      <td>lmk</td>\n",
       "      <td>thach</td>\n",
       "      <td>daston</td>\n",
       "      <td>angela</td>\n",
       "      <td>baby</td>\n",
       "      <td>mao</td>\n",
       "      <td>tru</td>\n",
       "      <td>icml</td>\n",
       "      <td>whatchu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word 1      Word 2     Word 3   Word 4    Word 5    Word 6  \\\n",
       "Year                                                                 \n",
       "2011    putnam         tht        dnt     yuna      thts     caleb   \n",
       "2012    burial      georgy    funeral   norton  boarders  smallnum   \n",
       "2013      moai        nawh      donna  digipen   warrant    meixia   \n",
       "2014  lemoneda  craneshout      unity    donna      nawh       wot   \n",
       "2015      nick        eric  vaporwave     tony    thipok   sticker   \n",
       "2016      nate       robin   terrance  beatrix       dan     tendi   \n",
       "2017  accuracy       perry        wya   jarrid      lmao    jeehee   \n",
       "2018    jarrid         wya        hug     lmao     anton    barzan   \n",
       "\n",
       "             Word 7        Word 8   Word 9      Word 10   Word 11   Word 12  \\\n",
       "Year                                                                          \n",
       "2011            kkk          guyz    kkkkk         jaee    doetze      judy   \n",
       "2012       olympiad         lizzy    nayan       zaahid      bowl   crannet   \n",
       "2013            mcs  snowboarding   damian  subramanian     peace  julicher   \n",
       "2014            lsd          domo   arviso      crannet   sticker       dcd   \n",
       "2015  equipotential         drone    ryuni     seizures   recital     kyoto   \n",
       "2016         tender          gord  octopus         lisa     tahoe      dolo   \n",
       "2017        entropy           sia      lmk      whatchu  cristina   branden   \n",
       "2018            lmk         thach   daston       angela      baby       mao   \n",
       "\n",
       "       Word 13      Word 14   Word 15  \n",
       "Year                                   \n",
       "2011       kay         ding  campused  \n",
       "2012     caleb         scio      gump  \n",
       "2013      deng       kinect       php  \n",
       "2014    chayce          dvc      rust  \n",
       "2015     canon     isoceles  epilepsy  \n",
       "2016   mission       bootie     perry  \n",
       "2017  giuseppe  autoencoder     theta  \n",
       "2018       tru         icml   whatchu  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(inbound_words,\n",
    "             columns=['Year'] + ['Word {}'.format(i+1) for i in range(NUM_WORDS)]) \\\n",
    "    .set_index('Year')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
